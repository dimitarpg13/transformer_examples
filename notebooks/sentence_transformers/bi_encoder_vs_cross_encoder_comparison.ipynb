{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/transformer_examples/blob/main/notebooks/sentence_transformers/bi_encoder_vs_cross_encoder_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtIPBFd91xw9"
      },
      "source": [
        "# Bi-Encoders vs Cross-Encoders: Comprehensive Comparison and Use Cases\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook provides a detailed comparison between bi-encoders and cross-encoders, demonstrating:\n",
        "- Architectural differences and computational trade-offs\n",
        "- Practical implementations using Sentence Transformers\n",
        "- Contrastive examples showing optimal use cases for each\n",
        "- Performance benchmarking and hybrid approaches\n",
        "- Production deployment considerations\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "**Bi-Encoders:**\n",
        "- Encode queries and documents independently\n",
        "- Enable pre-computation and caching of embeddings\n",
        "- Fast similarity search via vector operations\n",
        "- Suitable for large-scale retrieval\n",
        "\n",
        "**Cross-Encoders:**\n",
        "- Process query-document pairs jointly\n",
        "- Cannot pre-compute representations\n",
        "- More accurate relevance scoring\n",
        "- Suitable for re-ranking small candidate sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dkHQ9W81xw_"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "!pip install -q sentence-transformers faiss-cpu numpy pandas matplotlib seaborn tqdm\n",
        "!pip install -q datasets torch torchvision scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7rJ5ln01xw_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import time\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "import torch\n",
        "import faiss\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import ndcg_score, average_precision_score\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVX6Hxfz1xxA"
      },
      "source": [
        "## 1. Bi-Encoder Implementation\n",
        "\n",
        "Bi-encoders encode queries and documents separately, enabling efficient similarity search through vector operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dkr_Qw61xxA"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BiEncoderConfig:\n",
        "    \"\"\"Configuration for bi-encoder models\"\"\"\n",
        "    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "    embedding_dim: int = 384\n",
        "    max_seq_length: int = 256\n",
        "    normalize_embeddings: bool = True\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    batch_size: int = 32\n",
        "\n",
        "class BiEncoderRetriever:\n",
        "    \"\"\"Production-ready bi-encoder retriever with FAISS indexing\"\"\"\n",
        "\n",
        "    def __init__(self, config: BiEncoderConfig):\n",
        "        self.config = config\n",
        "        self.model = SentenceTransformer(config.model_name)\n",
        "        self.model.max_seq_length = config.max_seq_length\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "        self.embeddings = None\n",
        "\n",
        "        logger.info(f\"Initialized bi-encoder: {config.model_name}\")\n",
        "\n",
        "    def encode_documents(self, documents: List[str], show_progress: bool = True) -> np.ndarray:\n",
        "        \"\"\"Encode documents into embeddings\"\"\"\n",
        "        logger.info(f\"Encoding {len(documents)} documents...\")\n",
        "\n",
        "        self.documents = documents\n",
        "        self.embeddings = self.model.encode(\n",
        "            documents,\n",
        "            convert_to_tensor=False,\n",
        "            batch_size=self.config.batch_size,\n",
        "            show_progress_bar=show_progress,\n",
        "            normalize_embeddings=self.config.normalize_embeddings\n",
        "        )\n",
        "\n",
        "        # Build FAISS index\n",
        "        self._build_index()\n",
        "\n",
        "        return self.embeddings\n",
        "\n",
        "    def _build_index(self):\n",
        "        \"\"\"Build FAISS index for efficient similarity search\"\"\"\n",
        "        if self.config.normalize_embeddings:\n",
        "            # Use Inner Product for normalized vectors (equivalent to cosine similarity)\n",
        "            self.index = faiss.IndexFlatIP(self.config.embedding_dim)\n",
        "        else:\n",
        "            # Use L2 distance for non-normalized vectors\n",
        "            self.index = faiss.IndexFlatL2(self.config.embedding_dim)\n",
        "\n",
        "        self.index.add(self.embeddings.astype('float32'))\n",
        "        logger.info(f\"Built FAISS index with {self.index.ntotal} vectors\")\n",
        "\n",
        "    def search(self, queries: Union[str, List[str]], top_k: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search for most similar documents\"\"\"\n",
        "        if isinstance(queries, str):\n",
        "            queries = [queries]\n",
        "\n",
        "        # Encode queries\n",
        "        query_embeddings = self.model.encode(\n",
        "            queries,\n",
        "            convert_to_tensor=False,\n",
        "            normalize_embeddings=self.config.normalize_embeddings\n",
        "        )\n",
        "\n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embeddings.astype('float32'), top_k)\n",
        "\n",
        "        results = []\n",
        "        for q_idx, query in enumerate(queries):\n",
        "            query_results = {\n",
        "                'query': query,\n",
        "                'results': [\n",
        "                    {\n",
        "                        'document': self.documents[idx],\n",
        "                        'score': float(score),\n",
        "                        'rank': rank + 1\n",
        "                    }\n",
        "                    for rank, (idx, score) in enumerate(zip(indices[q_idx], scores[q_idx]))\n",
        "                    if idx != -1  # FAISS returns -1 for empty results\n",
        "                ]\n",
        "            }\n",
        "            results.append(query_results)\n",
        "\n",
        "        return results[0] if len(queries) == 1 else results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e4BmiKK1xxB"
      },
      "source": [
        "## 2. Cross-Encoder Implementation\n",
        "\n",
        "Cross-encoders process query-document pairs jointly for more accurate relevance scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDPNSzic1xxB"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CrossEncoderConfig:\n",
        "    \"\"\"Configuration for cross-encoder models\"\"\"\n",
        "    model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "    max_length: int = 512\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    batch_size: int = 16\n",
        "    activation_fct: str = 'sigmoid'  # or 'none' for raw scores\n",
        "\n",
        "class CrossEncoderReranker:\n",
        "    \"\"\"Production-ready cross-encoder reranker\"\"\"\n",
        "\n",
        "    def __init__(self, config: CrossEncoderConfig):\n",
        "        self.config = config\n",
        "        self.model = CrossEncoder(\n",
        "            config.model_name,\n",
        "            max_length=config.max_length,\n",
        "            device=config.device\n",
        "        )\n",
        "        logger.info(f\"Initialized cross-encoder: {config.model_name}\")\n",
        "\n",
        "    def rerank(self, query: str, documents: List[str],\n",
        "               initial_scores: Optional[List[float]] = None) -> List[Dict]:\n",
        "        \"\"\"Rerank documents for a query\"\"\"\n",
        "\n",
        "        # Create query-document pairs\n",
        "        pairs = [[query, doc] for doc in documents]\n",
        "\n",
        "        # Get cross-encoder scores\n",
        "        ce_scores = self.model.predict(\n",
        "            pairs,\n",
        "            batch_size=self.config.batch_size,\n",
        "            show_progress_bar=False,\n",
        "            activation_fct=self.config.activation_fct\n",
        "        )\n",
        "\n",
        "        # Combine with initial scores if provided (optional hybrid scoring)\n",
        "        if initial_scores is not None:\n",
        "            # Normalize initial scores to [0, 1] range\n",
        "            min_score = min(initial_scores)\n",
        "            max_score = max(initial_scores)\n",
        "            if max_score > min_score:\n",
        "                norm_initial = [(s - min_score) / (max_score - min_score) for s in initial_scores]\n",
        "            else:\n",
        "                norm_initial = initial_scores\n",
        "\n",
        "            # Weighted combination (can be tuned)\n",
        "            alpha = 0.7  # Weight for cross-encoder scores\n",
        "            final_scores = [\n",
        "                alpha * ce_score + (1 - alpha) * init_score\n",
        "                for ce_score, init_score in zip(ce_scores, norm_initial)\n",
        "            ]\n",
        "        else:\n",
        "            final_scores = ce_scores\n",
        "\n",
        "        # Sort by scores\n",
        "        results = [\n",
        "            {\n",
        "                'document': doc,\n",
        "                'cross_encoder_score': float(ce_score),\n",
        "                'final_score': float(final_score),\n",
        "                'rank': rank + 1\n",
        "            }\n",
        "            for rank, (doc, ce_score, final_score) in enumerate(\n",
        "                sorted(zip(documents, ce_scores, final_scores),\n",
        "                       key=lambda x: x[2], reverse=True)\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def batch_rerank(self, queries: List[str], documents_list: List[List[str]]) -> List[List[Dict]]:\n",
        "        \"\"\"Rerank multiple queries efficiently\"\"\"\n",
        "        results = []\n",
        "        for query, documents in tqdm(zip(queries, documents_list),\n",
        "                                     total=len(queries), desc=\"Reranking\"):\n",
        "            results.append(self.rerank(query, documents))\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeCfyh6A1xxB"
      },
      "source": [
        "## 3. Contrastive Examples: When to Use Each\n",
        "\n",
        "Let's create realistic scenarios that highlight the strengths of each approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpcVjPPO1xxB"
      },
      "outputs": [],
      "source": [
        "# Create sample datasets for different use cases\n",
        "\n",
        "# Use Case 1: Large-scale document retrieval (Bi-encoder strength)\n",
        "technical_documents = [\n",
        "    \"RAG systems combine retrieval mechanisms with generative models to provide grounded responses.\",\n",
        "    \"Vector databases like Pinecone and Weaviate enable efficient similarity search at scale.\",\n",
        "    \"LangChain provides abstractions for building LLM-powered applications with retrieval.\",\n",
        "    \"Embedding models convert text into dense vector representations for semantic search.\",\n",
        "    \"Cross-attention mechanisms allow models to attend to external knowledge during generation.\",\n",
        "    \"Hybrid search combines dense and sparse retrieval methods for improved accuracy.\",\n",
        "    \"Knowledge graphs provide structured representations of entities and relationships.\",\n",
        "    \"Fine-tuning embedding models on domain-specific data improves retrieval quality.\",\n",
        "    \"Chunking strategies affect the granularity of retrieved information in RAG systems.\",\n",
        "    \"Reranking with cross-encoders improves precision at the cost of computational efficiency.\",\n",
        "    \"GraphRAG extends traditional RAG with graph-based knowledge representation.\",\n",
        "    \"Multi-hop reasoning requires iterative retrieval and reasoning steps.\",\n",
        "    \"Semantic caching reduces latency by storing embeddings of frequent queries.\",\n",
        "    \"Dense passage retrieval outperforms BM25 on many question-answering benchmarks.\",\n",
        "    \"Contrastive learning improves the quality of learned embeddings for retrieval.\"\n",
        "]\n",
        "\n",
        "# Use Case 2: Nuanced similarity (Cross-encoder strength)\n",
        "nuanced_pairs = [\n",
        "    (\"How do transformers work?\", [\n",
        "        \"Transformers use self-attention to process sequences in parallel.\",\n",
        "        \"Electrical transformers convert voltage levels in power systems.\",\n",
        "        \"The transformer architecture revolutionized NLP in 2017.\",\n",
        "        \"Attention mechanisms allow models to focus on relevant parts of input.\",\n",
        "        \"BERT and GPT are both based on the transformer architecture.\"\n",
        "    ]),\n",
        "    (\"What are the benefits of exercise?\", [\n",
        "        \"Regular physical activity improves cardiovascular health and mental wellbeing.\",\n",
        "        \"Exercise can be challenging to maintain without proper motivation.\",\n",
        "        \"Many people exercise to lose weight and build muscle.\",\n",
        "        \"The benefits of exercise include reduced risk of chronic diseases.\",\n",
        "        \"Some exercises are better for flexibility while others build strength.\"\n",
        "    ])\n",
        "]\n",
        "\n",
        "# Use Case 3: FAQ matching (Bi-encoder strength)\n",
        "faq_documents = [\n",
        "    \"How do I reset my password? Click on 'Forgot Password' on the login page.\",\n",
        "    \"What payment methods do you accept? We accept credit cards, PayPal, and bank transfers.\",\n",
        "    \"How long does shipping take? Standard shipping takes 5-7 business days.\",\n",
        "    \"Can I return an item? Yes, returns are accepted within 30 days of purchase.\",\n",
        "    \"How do I track my order? Use the tracking number sent to your email.\",\n",
        "    \"Is international shipping available? Yes, we ship to over 50 countries.\",\n",
        "    \"How do I contact customer support? Email support@example.com or call 1-800-EXAMPLE.\",\n",
        "    \"What is your refund policy? Full refunds are provided for unused items.\",\n",
        "    \"Do you offer discounts? Check our promotions page for current offers.\",\n",
        "    \"How do I create an account? Click 'Sign Up' and fill in your details.\"\n",
        "]\n",
        "\n",
        "# Use Case 4: Semantic textual similarity (Cross-encoder strength)\n",
        "semantic_pairs = [\n",
        "    (\"The weather is beautiful today.\", \"It's such a lovely day outside.\"),\n",
        "    (\"The weather is beautiful today.\", \"The forecast shows rain all week.\"),\n",
        "    (\"I need to buy groceries.\", \"Time to go shopping for food.\"),\n",
        "    (\"I need to buy groceries.\", \"The store closes at 9 PM.\"),\n",
        "    (\"The meeting was productive.\", \"We accomplished a lot in the meeting.\"),\n",
        "    (\"The meeting was productive.\", \"The meeting lasted two hours.\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s_ilcoW1xxC"
      },
      "source": [
        "## 4. Performance Comparison: Speed vs Accuracy Trade-offs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL4wI37x1xxC"
      },
      "outputs": [],
      "source": [
        "class PerformanceBenchmark:\n",
        "    \"\"\"Compare bi-encoder and cross-encoder performance\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.bi_encoder = BiEncoderRetriever(BiEncoderConfig())\n",
        "        self.cross_encoder = CrossEncoderReranker(CrossEncoderConfig())\n",
        "        self.results = {}\n",
        "\n",
        "    def benchmark_retrieval_speed(self, documents: List[str], queries: List[str], top_k: int = 10):\n",
        "        \"\"\"Compare retrieval speed\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SPEED BENCHMARK: Retrieval Performance\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Bi-encoder: Encoding + Search\n",
        "        print(\"\\n1. Bi-Encoder Performance:\")\n",
        "\n",
        "        # Document encoding (one-time cost)\n",
        "        start_time = time.time()\n",
        "        self.bi_encoder.encode_documents(documents, show_progress=False)\n",
        "        encoding_time = time.time() - start_time\n",
        "        print(f\"   Document encoding time: {encoding_time:.3f}s for {len(documents)} docs\")\n",
        "        print(f\"   Average per document: {encoding_time/len(documents)*1000:.2f}ms\")\n",
        "\n",
        "        # Query search (recurring cost)\n",
        "        start_time = time.time()\n",
        "        bi_results = [self.bi_encoder.search(q, top_k=top_k) for q in queries]\n",
        "        search_time = time.time() - start_time\n",
        "        print(f\"   Query search time: {search_time:.3f}s for {len(queries)} queries\")\n",
        "        print(f\"   Average per query: {search_time/len(queries)*1000:.2f}ms\")\n",
        "\n",
        "        # Cross-encoder: Direct scoring\n",
        "        print(\"\\n2. Cross-Encoder Performance:\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        ce_results = []\n",
        "        for query in queries:\n",
        "            # Score all documents (no pre-computation possible)\n",
        "            ce_results.append(self.cross_encoder.rerank(query, documents)[:top_k])\n",
        "        ce_time = time.time() - start_time\n",
        "        print(f\"   Total scoring time: {ce_time:.3f}s for {len(queries)} queries\")\n",
        "        print(f\"   Average per query: {ce_time/len(queries)*1000:.2f}ms\")\n",
        "\n",
        "        # Comparison\n",
        "        print(\"\\n3. Performance Comparison:\")\n",
        "        print(f\"   Bi-encoder (excluding encoding): {search_time/len(queries)*1000:.2f}ms per query\")\n",
        "        print(f\"   Cross-encoder: {ce_time/len(queries)*1000:.2f}ms per query\")\n",
        "        print(f\"   Speed ratio: {ce_time/search_time:.1f}x slower\")\n",
        "\n",
        "        # Store results\n",
        "        self.results['speed'] = {\n",
        "            'bi_encoder': {'encoding': encoding_time, 'search': search_time},\n",
        "            'cross_encoder': {'total': ce_time},\n",
        "            'queries': len(queries),\n",
        "            'documents': len(documents)\n",
        "        }\n",
        "\n",
        "        return bi_results, ce_results\n",
        "\n",
        "    def benchmark_accuracy(self, test_pairs: List[Tuple[str, str]],\n",
        "                          negative_pairs: List[Tuple[str, str]]):\n",
        "        \"\"\"Compare accuracy on semantic similarity task\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ACCURACY BENCHMARK: Semantic Similarity\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        all_pairs = test_pairs + negative_pairs\n",
        "        labels = [1] * len(test_pairs) + [0] * len(negative_pairs)\n",
        "\n",
        "        # Bi-encoder scores\n",
        "        bi_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        embeddings1 = bi_model.encode([p[0] for p in all_pairs])\n",
        "        embeddings2 = bi_model.encode([p[1] for p in all_pairs])\n",
        "        bi_scores = util.cos_sim(embeddings1, embeddings2).diagonal().numpy()\n",
        "\n",
        "        # Cross-encoder scores\n",
        "        ce_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "        ce_scores = ce_model.predict(all_pairs)\n",
        "\n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "        bi_auc = roc_auc_score(labels, bi_scores)\n",
        "        ce_auc = roc_auc_score(labels, ce_scores)\n",
        "\n",
        "        print(f\"\\n1. ROC-AUC Scores:\")\n",
        "        print(f\"   Bi-encoder: {bi_auc:.3f}\")\n",
        "        print(f\"   Cross-encoder: {ce_auc:.3f}\")\n",
        "        print(f\"   Improvement: {(ce_auc - bi_auc) * 100:.1f}%\")\n",
        "\n",
        "        # Threshold-based accuracy\n",
        "        bi_preds = (bi_scores > 0.5).astype(int)\n",
        "        ce_preds = (ce_scores > 0.5).astype(int)\n",
        "\n",
        "        bi_acc = accuracy_score(labels, bi_preds)\n",
        "        ce_acc = accuracy_score(labels, ce_preds)\n",
        "\n",
        "        print(f\"\\n2. Binary Classification Accuracy:\")\n",
        "        print(f\"   Bi-encoder: {bi_acc:.3f}\")\n",
        "        print(f\"   Cross-encoder: {ce_acc:.3f}\")\n",
        "\n",
        "        self.results['accuracy'] = {\n",
        "            'bi_encoder': {'auc': bi_auc, 'accuracy': bi_acc},\n",
        "            'cross_encoder': {'auc': ce_auc, 'accuracy': ce_acc}\n",
        "        }\n",
        "\n",
        "        return bi_scores, ce_scores, labels\n",
        "\n",
        "# Run benchmarks\n",
        "benchmark = PerformanceBenchmark()\n",
        "\n",
        "# Speed benchmark\n",
        "test_queries = [\n",
        "    \"How does RAG improve LLM responses?\",\n",
        "    \"What is semantic search?\",\n",
        "    \"Explain vector databases\"\n",
        "]\n",
        "\n",
        "bi_results, ce_results = benchmark.benchmark_retrieval_speed(\n",
        "    technical_documents,\n",
        "    test_queries,\n",
        "    top_k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4rsvNFg1xxC"
      },
      "source": [
        "## 5. Hybrid Approach: Two-Stage Retrieval Pipeline\n",
        "\n",
        "Combine bi-encoders for initial retrieval with cross-encoders for reranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3iyKuFC1xxC"
      },
      "outputs": [],
      "source": [
        "class HybridRetriever:\n",
        "    \"\"\"Two-stage retrieval: Bi-encoder retrieval + Cross-encoder reranking\"\"\"\n",
        "\n",
        "    def __init__(self, bi_encoder_config: BiEncoderConfig = None,\n",
        "                 cross_encoder_config: CrossEncoderConfig = None):\n",
        "        self.bi_encoder = BiEncoderRetriever(bi_encoder_config or BiEncoderConfig())\n",
        "        self.cross_encoder = CrossEncoderReranker(cross_encoder_config or CrossEncoderConfig())\n",
        "        self.metrics = {'retrieval_time': [], 'rerank_time': [], 'total_time': []}\n",
        "\n",
        "    def index_documents(self, documents: List[str]):\n",
        "        \"\"\"Index documents for retrieval\"\"\"\n",
        "        self.bi_encoder.encode_documents(documents)\n",
        "        logger.info(f\"Indexed {len(documents)} documents\")\n",
        "\n",
        "    def retrieve(self, query: str, initial_top_k: int = 100, final_top_k: int = 10,\n",
        "                 rerank_top_n: Optional[int] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Two-stage retrieval pipeline\n",
        "\n",
        "        Args:\n",
        "            query: Search query\n",
        "            initial_top_k: Number of candidates from bi-encoder\n",
        "            final_top_k: Final number of results after reranking\n",
        "            rerank_top_n: Number of top candidates to rerank (default: all)\n",
        "        \"\"\"\n",
        "        total_start = time.time()\n",
        "\n",
        "        # Stage 1: Fast retrieval with bi-encoder\n",
        "        retrieval_start = time.time()\n",
        "        initial_results = self.bi_encoder.search(query, top_k=initial_top_k)\n",
        "        retrieval_time = time.time() - retrieval_start\n",
        "\n",
        "        # Extract candidates for reranking\n",
        "        candidates = initial_results['results']\n",
        "        if rerank_top_n:\n",
        "            candidates = candidates[:rerank_top_n]\n",
        "\n",
        "        candidate_docs = [r['document'] for r in candidates]\n",
        "        initial_scores = [r['score'] for r in candidates]\n",
        "\n",
        "        # Stage 2: Accurate reranking with cross-encoder\n",
        "        rerank_start = time.time()\n",
        "        reranked = self.cross_encoder.rerank(query, candidate_docs, initial_scores)\n",
        "        rerank_time = time.time() - rerank_start\n",
        "\n",
        "        # Select top-k after reranking\n",
        "        final_results = reranked[:final_top_k]\n",
        "\n",
        "        total_time = time.time() - total_start\n",
        "\n",
        "        # Store metrics\n",
        "        self.metrics['retrieval_time'].append(retrieval_time)\n",
        "        self.metrics['rerank_time'].append(rerank_time)\n",
        "        self.metrics['total_time'].append(total_time)\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'results': final_results,\n",
        "            'metrics': {\n",
        "                'retrieval_time': retrieval_time,\n",
        "                'rerank_time': rerank_time,\n",
        "                'total_time': total_time,\n",
        "                'initial_candidates': len(candidates),\n",
        "                'final_results': len(final_results)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def evaluate_pipeline(self, queries: List[str], relevant_docs: List[List[int]],\n",
        "                         initial_top_k: int = 100, final_top_k: int = 10):\n",
        "        \"\"\"Evaluate the hybrid pipeline with metrics\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for query, relevant in tqdm(zip(queries, relevant_docs), total=len(queries)):\n",
        "            result = self.retrieve(query, initial_top_k, final_top_k)\n",
        "\n",
        "            # Calculate metrics\n",
        "            retrieved_indices = [self.bi_encoder.documents.index(r['document'])\n",
        "                               for r in result['results']\n",
        "                               if r['document'] in self.bi_encoder.documents]\n",
        "\n",
        "            # Precision@k\n",
        "            hits = sum(1 for idx in retrieved_indices[:final_top_k] if idx in relevant)\n",
        "            precision = hits / min(final_top_k, len(retrieved_indices))\n",
        "\n",
        "            # Recall@k\n",
        "            recall = hits / len(relevant) if relevant else 0\n",
        "\n",
        "            result['evaluation'] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        # Aggregate metrics\n",
        "        avg_metrics = {\n",
        "            'avg_precision': np.mean([r['evaluation']['precision'] for r in results]),\n",
        "            'avg_recall': np.mean([r['evaluation']['recall'] for r in results]),\n",
        "            'avg_f1': np.mean([r['evaluation']['f1'] for r in results]),\n",
        "            'avg_retrieval_time': np.mean(self.metrics['retrieval_time']),\n",
        "            'avg_rerank_time': np.mean(self.metrics['rerank_time']),\n",
        "            'avg_total_time': np.mean(self.metrics['total_time'])\n",
        "        }\n",
        "\n",
        "        return results, avg_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RysGzpRK1xxD"
      },
      "source": [
        "## 6. Practical Use Case Demonstrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m2SW7cG1xxD"
      },
      "outputs": [],
      "source": [
        "def demonstrate_use_cases():\n",
        "    \"\"\"Demonstrate when to use each approach\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"USE CASE DEMONSTRATIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Use Case 1: Large-scale FAQ Search (Bi-encoder optimal)\n",
        "    print(\"\\n\" + \"─\"*70)\n",
        "    print(\"USE CASE 1: FAQ Search System (10,000+ FAQs)\")\n",
        "    print(\"OPTIMAL: Bi-encoder (pre-computed embeddings + fast search)\")\n",
        "    print(\"─\"*70)\n",
        "\n",
        "    bi_config = BiEncoderConfig(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "    faq_retriever = BiEncoderRetriever(bi_config)\n",
        "\n",
        "    # Simulate large FAQ database\n",
        "    large_faq_db = faq_documents * 100  # 1000 FAQs\n",
        "    print(f\"\\nIndexing {len(large_faq_db)} FAQs...\")\n",
        "\n",
        "    start = time.time()\n",
        "    faq_retriever.encode_documents(large_faq_db, show_progress=False)\n",
        "    print(f\"Indexing completed in {time.time() - start:.2f}s\")\n",
        "\n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"How can I get my money back?\",\n",
        "        \"I forgot my login credentials\",\n",
        "        \"Where is my package?\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nQuery results:\")\n",
        "    for query in test_queries:\n",
        "        start = time.time()\n",
        "        results = faq_retriever.search(query, top_k=3)\n",
        "        query_time = (time.time() - start) * 1000\n",
        "\n",
        "        print(f\"\\nQuery: '{query}' (Time: {query_time:.1f}ms)\")\n",
        "        for r in results['results'][:2]:\n",
        "            print(f\"  - Score: {r['score']:.3f} | {r['document'][:60]}...\")\n",
        "\n",
        "    # Use Case 2: Legal Document Ranking (Cross-encoder optimal)\n",
        "    print(\"\\n\" + \"─\"*70)\n",
        "    print(\"USE CASE 2: Legal Document Relevance (High Precision Required)\")\n",
        "    print(\"OPTIMAL: Cross-encoder (nuanced understanding needed)\")\n",
        "    print(\"─\"*70)\n",
        "\n",
        "    legal_documents = [\n",
        "        \"The defendant breached the contract by failing to deliver goods on the agreed date.\",\n",
        "        \"Contract law requires mutual consideration for a valid agreement.\",\n",
        "        \"The breach resulted in significant financial damages to the plaintiff.\",\n",
        "        \"Force majeure clauses may excuse performance under certain circumstances.\",\n",
        "        \"The court found that the contract was unconscionable and therefore unenforceable.\"\n",
        "    ]\n",
        "\n",
        "    legal_query = \"Was there a material breach that caused monetary harm?\"\n",
        "\n",
        "    ce_config = CrossEncoderConfig(model_name='cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
        "    legal_reranker = CrossEncoderReranker(ce_config)\n",
        "\n",
        "    print(f\"\\nQuery: '{legal_query}'\")\n",
        "    print(\"\\nCross-encoder relevance scores:\")\n",
        "\n",
        "    results = legal_reranker.rerank(legal_query, legal_documents)\n",
        "    for r in results:\n",
        "        print(f\"  Score: {r['cross_encoder_score']:.3f} | {r['document'][:70]}...\")\n",
        "\n",
        "    # Use Case 3: E-commerce Search (Hybrid approach optimal)\n",
        "    print(\"\\n\" + \"─\"*70)\n",
        "    print(\"USE CASE 3: E-commerce Product Search (Speed + Relevance)\")\n",
        "    print(\"OPTIMAL: Hybrid (Bi-encoder retrieval + Cross-encoder reranking)\")\n",
        "    print(\"─\"*70)\n",
        "\n",
        "    product_catalog = [\n",
        "        \"Sony WH-1000XM4 Wireless Noise Canceling Headphones - Black\",\n",
        "        \"Bose QuietComfort 35 II Wireless Bluetooth Headphones\",\n",
        "        \"Apple AirPods Pro with Active Noise Cancellation\",\n",
        "        \"Beats Studio3 Wireless Over-Ear Headphones - Matte Black\",\n",
        "        \"Sennheiser HD 650 Open Back Professional Headphones\",\n",
        "        \"Audio-Technica ATH-M50x Professional Studio Monitor Headphones\",\n",
        "        \"JBL Tune 750BTNC Wireless Over-Ear Headphones with Noise Cancellation\",\n",
        "        \"Anker Soundcore Life Q20 Hybrid Active Noise Cancelling Headphones\",\n",
        "        \"Skullcandy Crusher Wireless Over-Ear Headphones with Bass\",\n",
        "        \"Plantronics BackBeat Pro 2 Wireless Noise Cancelling Headphones\"\n",
        "    ] * 50  # Simulate 500 products\n",
        "\n",
        "    hybrid = HybridRetriever()\n",
        "    hybrid.index_documents(product_catalog)\n",
        "\n",
        "    search_query = \"wireless headphones with best noise cancellation under $300\"\n",
        "\n",
        "    print(f\"\\nSearch query: '{search_query}'\")\n",
        "    result = hybrid.retrieve(search_query, initial_top_k=20, final_top_k=5)\n",
        "\n",
        "    print(f\"\\nPipeline metrics:\")\n",
        "    print(f\"  Stage 1 (Bi-encoder): {result['metrics']['retrieval_time']*1000:.1f}ms\")\n",
        "    print(f\"  Stage 2 (Cross-encoder reranking): {result['metrics']['rerank_time']*1000:.1f}ms\")\n",
        "    print(f\"  Total time: {result['metrics']['total_time']*1000:.1f}ms\")\n",
        "\n",
        "    print(f\"\\nTop results:\")\n",
        "    for i, r in enumerate(result['results'][:3], 1):\n",
        "        print(f\"  {i}. Score: {r['final_score']:.3f} | {r['document'][:60]}...\")\n",
        "\n",
        "# Run demonstrations\n",
        "demonstrate_use_cases()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARTpokUi1xxD"
      },
      "source": [
        "## 7. Decision Framework: Choosing the Right Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_JfQWhC1xxD"
      },
      "outputs": [],
      "source": [
        "def create_decision_framework():\n",
        "    \"\"\"Create a decision framework for choosing between approaches\"\"\"\n",
        "\n",
        "    framework = pd.DataFrame({\n",
        "        'Scenario': [\n",
        "            'Large-scale search (>10K docs)',\n",
        "            'Real-time search requirements',\n",
        "            'High precision needed',\n",
        "            'Complex semantic understanding',\n",
        "            'FAQ/Knowledge base search',\n",
        "            'Document similarity at scale',\n",
        "            'Passage ranking for QA',\n",
        "            'Legal/Medical document ranking',\n",
        "            'E-commerce search',\n",
        "            'Semantic textual similarity',\n",
        "            'Information retrieval first-stage',\n",
        "            'Re-ranking top candidates',\n",
        "            'Clustering/Classification',\n",
        "            'Zero-shot classification'\n",
        "        ],\n",
        "        'Recommended': [\n",
        "            'Bi-encoder',\n",
        "            'Bi-encoder',\n",
        "            'Cross-encoder',\n",
        "            'Cross-encoder',\n",
        "            'Bi-encoder',\n",
        "            'Bi-encoder',\n",
        "            'Cross-encoder',\n",
        "            'Cross-encoder',\n",
        "            'Hybrid',\n",
        "            'Cross-encoder',\n",
        "            'Bi-encoder',\n",
        "            'Cross-encoder',\n",
        "            'Bi-encoder',\n",
        "            'Cross-encoder'\n",
        "        ],\n",
        "        'Reasoning': [\n",
        "            'Pre-computed embeddings enable sub-second search',\n",
        "            'Fast vector similarity search meets latency requirements',\n",
        "            'Joint encoding captures nuanced relationships',\n",
        "            'Cross-attention enables deep semantic understanding',\n",
        "            'Efficient similarity search with cached embeddings',\n",
        "            'Scalable with approximate nearest neighbor search',\n",
        "            'Accurate relevance scoring for answer extraction',\n",
        "            'Precision critical; computational cost acceptable',\n",
        "            'Balance speed (bi) with relevance (cross)',\n",
        "            'Direct comparison yields highest accuracy',\n",
        "            'Efficient candidate generation from large corpus',\n",
        "            'Improve precision on manageable candidate set',\n",
        "            'Dense embeddings enable efficient clustering',\n",
        "            'Accurate classification without training data'\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"DECISION FRAMEWORK: When to Use Each Approach\")\n",
        "    print(\"=\"*100)\n",
        "    print(framework.to_string(index=False))\n",
        "\n",
        "    # Visualize trade-offs\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Speed vs Accuracy trade-off\n",
        "    approaches = ['Bi-encoder\\n(Retrieval)', 'Cross-encoder\\n(Reranking)', 'Hybrid\\n(Two-stage)']\n",
        "    speed = [95, 20, 75]  # Relative speed (higher is faster)\n",
        "    accuracy = [70, 95, 90]  # Relative accuracy\n",
        "\n",
        "    ax1.scatter(speed, accuracy, s=500, alpha=0.6, c=['blue', 'red', 'green'])\n",
        "    for i, txt in enumerate(approaches):\n",
        "        ax1.annotate(txt, (speed[i], accuracy[i]), ha='center', va='center')\n",
        "\n",
        "    ax1.set_xlabel('Speed (relative)', fontsize=12)\n",
        "    ax1.set_ylabel('Accuracy (relative)', fontsize=12)\n",
        "    ax1.set_title('Speed vs Accuracy Trade-off', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlim(0, 100)\n",
        "    ax1.set_ylim(60, 100)\n",
        "\n",
        "    # Scalability comparison\n",
        "    doc_counts = [100, 1000, 10000, 100000, 1000000]\n",
        "    bi_times = [0.01, 0.02, 0.05, 0.1, 0.2]  # Query times in seconds\n",
        "    ce_times = [0.1, 1, 10, 100, 1000]  # Cross-encoder doesn't scale\n",
        "    hybrid_times = [0.05, 0.1, 0.15, 0.2, 0.3]  # Hybrid approach\n",
        "\n",
        "    ax2.loglog(doc_counts, bi_times, 'b-o', label='Bi-encoder', linewidth=2)\n",
        "    ax2.loglog(doc_counts, ce_times, 'r-s', label='Cross-encoder', linewidth=2)\n",
        "    ax2.loglog(doc_counts, hybrid_times, 'g-^', label='Hybrid', linewidth=2)\n",
        "\n",
        "    ax2.set_xlabel('Number of Documents', fontsize=12)\n",
        "    ax2.set_ylabel('Query Time (seconds)', fontsize=12)\n",
        "    ax2.set_title('Scalability Comparison', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, which='both')\n",
        "    ax2.legend(loc='upper left')\n",
        "\n",
        "    # Add practical latency thresholds\n",
        "    ax2.axhline(y=0.1, color='orange', linestyle='--', alpha=0.5, label='100ms (good UX)')\n",
        "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='1s (acceptable)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return framework\n",
        "\n",
        "decision_framework = create_decision_framework()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s86Dq10i1xxE"
      },
      "source": [
        "## 8. Production Deployment Considerations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he1T_kZ91xxE"
      },
      "outputs": [],
      "source": [
        "class ProductionGuidelines:\n",
        "    \"\"\"Production deployment guidelines and monitoring\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def print_deployment_checklist():\n",
        "        \"\"\"Print deployment considerations for each approach\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PRODUCTION DEPLOYMENT GUIDELINES\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n1. BI-ENCODER DEPLOYMENT\")\n",
        "        print(\"─\" * 40)\n",
        "        print(\"\"\"\n",
        "        Infrastructure Requirements:\n",
        "        - Vector database (Pinecone, Weaviate, Milvus, or FAISS)\n",
        "        - GPU for encoding (optional but recommended)\n",
        "        - Caching layer for frequent queries\n",
        "\n",
        "        Optimization Strategies:\n",
        "        - Batch encoding for document updates\n",
        "        - Approximate nearest neighbor (ANN) for large scale\n",
        "        - Quantization to reduce memory footprint\n",
        "        - Asynchronous embedding updates\n",
        "\n",
        "        Monitoring Metrics:\n",
        "        - Encoding latency (P50, P95, P99)\n",
        "        - Search latency\n",
        "        - Index size and memory usage\n",
        "        - Cache hit rate\n",
        "        \"\"\")\n",
        "\n",
        "        print(\"\\n2. CROSS-ENCODER DEPLOYMENT\")\n",
        "        print(\"─\" * 40)\n",
        "        print(\"\"\"\n",
        "        Infrastructure Requirements:\n",
        "        - GPU inference servers (strongly recommended)\n",
        "        - Model serving framework (TorchServe, Triton)\n",
        "        - Request batching system\n",
        "\n",
        "        Optimization Strategies:\n",
        "        - Dynamic batching for concurrent requests\n",
        "        - Model quantization (INT8/FP16)\n",
        "        - Result caching for repeated queries\n",
        "        - Distributed inference for high load\n",
        "\n",
        "        Monitoring Metrics:\n",
        "        - Inference latency per batch size\n",
        "        - GPU utilization\n",
        "        - Queue depth and wait times\n",
        "        - Model load distribution\n",
        "        \"\"\")\n",
        "\n",
        "        print(\"\\n3. HYBRID PIPELINE DEPLOYMENT\")\n",
        "        print(\"─\" * 40)\n",
        "        print(\"\"\"\n",
        "        Infrastructure Requirements:\n",
        "        - Vector database for bi-encoder\n",
        "        - GPU inference for cross-encoder\n",
        "        - Orchestration layer (e.g., Ray, Celery)\n",
        "        - Results caching infrastructure\n",
        "\n",
        "        Optimization Strategies:\n",
        "        - Adaptive candidate selection based on query complexity\n",
        "        - Parallel processing of reranking batches\n",
        "        - Progressive result streaming\n",
        "        - Smart caching of intermediate results\n",
        "\n",
        "        Monitoring Metrics:\n",
        "        - End-to-end latency breakdown\n",
        "        - Candidate set size distribution\n",
        "        - Reranking impact on relevance\n",
        "        - Resource utilization per stage\n",
        "        \"\"\")\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_monitoring_dashboard():\n",
        "        \"\"\"Generate sample monitoring metrics\"\"\"\n",
        "\n",
        "        # Simulate metrics over time\n",
        "        hours = np.arange(24)\n",
        "\n",
        "        # Bi-encoder metrics\n",
        "        bi_latency = 20 + 5 * np.sin(hours * np.pi / 12) + np.random.normal(0, 2, 24)\n",
        "        bi_qps = 1000 + 300 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 50, 24)\n",
        "\n",
        "        # Cross-encoder metrics\n",
        "        ce_latency = 200 + 50 * np.sin(hours * np.pi / 12) + np.random.normal(0, 20, 24)\n",
        "        ce_qps = 100 + 30 * np.sin((hours - 6) * np.pi / 12) + np.random.normal(0, 10, 24)\n",
        "\n",
        "        # Create dashboard\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        # Latency comparison\n",
        "        ax1.plot(hours, bi_latency, 'b-', label='Bi-encoder', linewidth=2)\n",
        "        ax1.plot(hours, ce_latency, 'r-', label='Cross-encoder', linewidth=2)\n",
        "        ax1.set_xlabel('Hour of Day')\n",
        "        ax1.set_ylabel('Latency (ms)')\n",
        "        ax1.set_title('Query Latency Over Time', fontweight='bold')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # QPS comparison\n",
        "        ax2.plot(hours, bi_qps, 'b-', label='Bi-encoder', linewidth=2)\n",
        "        ax2.plot(hours, ce_qps, 'r-', label='Cross-encoder', linewidth=2)\n",
        "        ax2.set_xlabel('Hour of Day')\n",
        "        ax2.set_ylabel('Queries Per Second')\n",
        "        ax2.set_title('Throughput Over Time', fontweight='bold')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Cost efficiency\n",
        "        doc_counts = [1000, 10000, 100000, 1000000]\n",
        "        bi_cost = [0.01, 0.05, 0.2, 0.5]  # Dollars per 1000 queries\n",
        "        ce_cost = [0.1, 1, 10, 100]\n",
        "        hybrid_cost = [0.03, 0.15, 0.4, 0.8]\n",
        "\n",
        "        ax3.loglog(doc_counts, bi_cost, 'b-o', label='Bi-encoder', linewidth=2)\n",
        "        ax3.loglog(doc_counts, ce_cost, 'r-s', label='Cross-encoder', linewidth=2)\n",
        "        ax3.loglog(doc_counts, hybrid_cost, 'g-^', label='Hybrid', linewidth=2)\n",
        "        ax3.set_xlabel('Corpus Size')\n",
        "        ax3.set_ylabel('Cost per 1000 Queries ($)')\n",
        "        ax3.set_title('Cost Efficiency Analysis', fontweight='bold')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3, which='both')\n",
        "\n",
        "        # Accuracy vs latency trade-off for different configurations\n",
        "        configs = [\n",
        "            ('Bi-encoder\\nOnly', 20, 0.75),\n",
        "            ('Cross-encoder\\nOnly', 200, 0.92),\n",
        "            ('Hybrid\\nTop-100', 40, 0.88),\n",
        "            ('Hybrid\\nTop-50', 35, 0.86),\n",
        "            ('Hybrid\\nTop-20', 30, 0.83),\n",
        "        ]\n",
        "\n",
        "        for config, latency, accuracy in configs:\n",
        "            ax4.scatter(latency, accuracy, s=200, alpha=0.7)\n",
        "            ax4.annotate(config, (latency, accuracy), ha='center', fontsize=9)\n",
        "\n",
        "        ax4.set_xlabel('Latency (ms)')\n",
        "        ax4.set_ylabel('Accuracy (nDCG@10)')\n",
        "        ax4.set_title('Configuration Trade-offs', fontweight='bold')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.suptitle('Production Monitoring Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generate production guidelines\n",
        "guidelines = ProductionGuidelines()\n",
        "guidelines.print_deployment_checklist()\n",
        "guidelines.generate_monitoring_dashboard()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8keN9FM1xxE"
      },
      "source": [
        "## 9. Advanced Techniques and Best Practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW99kc9l1xxE"
      },
      "outputs": [],
      "source": [
        "# Advanced configuration examples\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADVANCED CONFIGURATION EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. DOMAIN-SPECIFIC MODEL SELECTION\n",
        "───────────────────────────────────\n",
        "\n",
        "Bi-Encoders:\n",
        "• General: 'sentence-transformers/all-mpnet-base-v2' (Best quality)\n",
        "• Fast: 'sentence-transformers/all-MiniLM-L6-v2' (5x faster, 95% quality)\n",
        "• Multilingual: 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "• Scientific: 'allenai/specter2' (Scientific papers)\n",
        "• Code: 'microsoft/codebert-base' (Code search)\n",
        "\n",
        "Cross-Encoders:\n",
        "• General: 'cross-encoder/ms-marco-MiniLM-L-12-v2' (Best balance)\n",
        "• High Accuracy: 'cross-encoder/ms-marco-electra-base' (Slower but accurate)\n",
        "• Fast: 'cross-encoder/ms-marco-TinyBERT-L-2-v2' (3x faster)\n",
        "• Multilingual: 'cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'\n",
        "\n",
        "2. OPTIMIZATION TECHNIQUES\n",
        "──────────────────────────\n",
        "\n",
        "Bi-Encoder Optimizations:\n",
        "• Use product quantization for large indexes (8x memory reduction)\n",
        "• Implement IVF (Inverted File) indexing for billion-scale search\n",
        "• Apply dimensionality reduction (PCA/UMAP) for faster search\n",
        "• Cache frequently accessed embeddings in Redis/Memcached\n",
        "\n",
        "Cross-Encoder Optimizations:\n",
        "• Quantize models to INT8 (2-4x speedup, <1% accuracy loss)\n",
        "• Use ONNX Runtime for optimized inference\n",
        "• Implement dynamic batching with padding\n",
        "• Deploy multiple model replicas with load balancing\n",
        "\n",
        "3. HYBRID PIPELINE TUNING\n",
        "─────────────────────────\n",
        "\n",
        "Adaptive Strategies:\n",
        "• Adjust initial_top_k based on query complexity (10-200)\n",
        "• Use query classification to skip reranking for simple queries\n",
        "• Implement cascade reranking with multiple cross-encoders\n",
        "• Cache reranking results for popular query patterns\n",
        "\n",
        "4. EVALUATION METRICS\n",
        "─────────────────────\n",
        "\n",
        "Retrieval Metrics:\n",
        "• MRR (Mean Reciprocal Rank): Position of first relevant result\n",
        "• nDCG@k: Graded relevance at position k\n",
        "• MAP (Mean Average Precision): Average precision across queries\n",
        "• Recall@k: Fraction of relevant documents retrieved\n",
        "\n",
        "Production Metrics:\n",
        "• P50/P95/P99 latencies\n",
        "• Queries per second (QPS)\n",
        "• GPU/CPU utilization\n",
        "• Memory consumption\n",
        "• Cache hit rates\n",
        "\"\"\")\n",
        "\n",
        "# Summary recommendations\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY: KEY RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "recommendations = pd.DataFrame({\n",
        "    'Use Case': [\n",
        "        'Semantic Search (1M+ docs)',\n",
        "        'Question Answering',\n",
        "        'Duplicate Detection',\n",
        "        'Document Ranking',\n",
        "        'Real-time Search',\n",
        "        'Clustering',\n",
        "        'Zero-shot Classification'\n",
        "    ],\n",
        "    'Approach': [\n",
        "        'Bi-encoder + FAISS',\n",
        "        'Hybrid (Bi + Cross)',\n",
        "        'Bi-encoder',\n",
        "        'Cross-encoder',\n",
        "        'Bi-encoder',\n",
        "        'Bi-encoder',\n",
        "        'Cross-encoder'\n",
        "    ],\n",
        "    'Key Consideration': [\n",
        "        'Pre-compute embeddings, use ANN index',\n",
        "        'Balance retrieval recall with ranking precision',\n",
        "        'Threshold on cosine similarity',\n",
        "        'Accuracy more important than speed',\n",
        "        'Latency critical, cache aggressively',\n",
        "        'Generate embeddings once, cluster offline',\n",
        "        'Fine-grained classification without training'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + recommendations.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Notebook execution completed successfully!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}